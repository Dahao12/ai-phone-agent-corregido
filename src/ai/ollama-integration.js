/**
 * Ollama Integration - LLM LOCAL GRATUITO
 * Reemplaza OpenAI GPT-4 con Llama 3.1 8B (100% gratis)
 */

const axios = require('axios');

class OllamaIntegration {
  constructor(config) {
    this.baseUrl = 'http://localhost:11434/api';
    this.model = config.ollama?.model || 'llama3.1:8b';
    this.temperature = config.ollama?.temperature || 0.7;
    this.callScript = null;
  }

  /**
   * Configura el guion de llamada
   */
  setCallScript(script) {
    this.callScript = script;
    console.log('‚úÖ Guion de llamada configurado en Ollama');
  }

  /**
   * GPT-4 Chat Alternativo - Llama 3.1 8B
   */
  async chatCompletion(messages, options = {}) {
    try {
      // Construir prompt de sistema
      let systemPrompt = this._getSystemPrompt();

      // A√±adir guion si existe
      if (this.callScript) {
        systemPrompt += `\n\nGUION BASE:\n${this.callScript}`;
      }

      // Formatear mensajes para Ollama
      const formattedMessages = [
        { role: 'system', content: systemPrompt },
        ...messages.map(msg => ({
          role: msg.role === 'assistant' ? 'assistant' : 'user',
          content: msg.content
        }))
      ];

      const response = await axios.post(
        `${this.baseUrl}/chat`,
        {
          model: this.model,
          messages: formattedMessages,
          stream: false,
          options: {
            temperature: options.temperature || this.temperature,
            num_predict: options.maxTokens || 150
          }
        },
        {
          headers: {
            'Content-Type': 'application/json'
          }
        }
      );

      const reply = response.data.message?.content || '';
      console.log(`ü§ñ Ollama: "${reply.substring(0, 100)}..."`);
      return reply;

    } catch (error) {
      console.error('Error en Ollama Chat:', error.response?.data || error.message);

      // Fallback: Respuesta por defecto si Ollama falla
      return this._getDefaultResponse();
    }
  }

  /**
   * Prompt de sistema para Llama 3.1
   */
  _getSystemPrompt() {
    let systemPrompt = `ERES UN ASESOR DE VENTAS PROFESIONAL DE ENERLUX SOLUCIONES, empresa de energ√≠a renovable en Espa√±a.

TUS RESPONSABILIDADES:
1. Llamada profesional y respetuosa
2. Ofrecer servicios de energ√≠a (electricidad + gas)
3. Identificar inter√©s del cliente
4. Manejar objeciones con empat√≠a
5. Ser conciso (m√°ximo 20 palabras por respuesta)

ESTILO DE COMUNICACI√ìN:
- Profesional pero cercano
- No agresivo ni insistente
- Escuchar activamente
- Proporcionar informaci√≥n clara
- Espa√±ol de Espa√±a

OBJETIVO:
Detectar si el cliente tiene interÔøΩs en cambiar de proveedor de energ√≠a.
Si S√ç ‚Üí Preguntar m√°s espec√≠ficamente sobre su situaci√≥n.
Si NO ‚Üí Aceptar rechazo y agradecer amablemente.

RESPONDEN EN ESPA√ëOL, M√ÅXIMO 20 PALABRAS.
`;

    return systemPrompt;
  }

  /**
   * Respuesta por defecto si Ollama falla
   */
  _getDefaultResponse() {
    const defaults = [
      'Perfecto, le entiendo. ¬øLe gustar√≠a m√°s informaci√≥n?',
      'Claro, le explico. ¬øC√≥mo le gustar√≠a proseguir?',
      'Entiendo, ¬øAlg√∫n momento que le convenga hablar?',
      'De nada, si tiene dudas me puede llamar. Un d√≠a.'
    ];

    return defaults[Math.floor(Math.random() * defaults.length)];
  }

  /**
   * Genera saludo inicial
   */
  async generateGreeting(clientName) {
    const messages = [
      {
        role: 'user',
        content: `Genera saludo inicial profesional para llamar a ${clientName}. Presenta Enerlux Soluciones y prop√≥sito de llamada. M√ÅXIMO 25 palabras.`
      }
    ];

    return await this.chatCompletion(messages, { maxTokens: 30 });
  }

  /**
   * Analiza respuesta del cliente
   */
  async analyzeClientResponse(clientResponse, clientName) {
    const messages = [
      {
        role: 'user',
        content: `El cliente ${clientName} respondi√≥: "${clientResponse}"

Genera respuesta profesional de asesor energ√©tico. M√°ximo 30 palabras. Si interesado, pregunta m√°s. Si rechaz√≥, agradece amablemente.`
      }
    ];

    return await this.chatCompletion(messages, { maxTokens: 40 });
  }

  /**
   * Determina outcome de llamada
   */
  async determineCallOutcome(conversationHistory) {
    const messages = [
      {
        role: 'user',
        content: `Basado en esta conversaci√≥n, determina si el cliente est√° INTERESTED o NOT_INTERESTED.

CONVERSACI√ìN:
${conversationHistory.map(msg => `${msg.role}: ${msg.content}`).join('\n')}

Responde solo una palabra: INTERESTED o NOT_INTERESTED`
      }
    ];

    try {
      const result = await this.chatCompletion(messages, { maxTokens: 5, temperature: 0.1 });
      return result.trim().toUpperCase().includes('INTERESTED') ? 'Interested' : 'Not Interested';
    } catch (error) {
      return 'Not Interested';
    }
  }

  /**
   * Verifica que Ollama est√° disponible
   */
  async checkConnection() {
    try {
      const response = await axios.get('http://localhost:11434/api/tags');
      console.log('‚úÖ Ollama est√° disponible');
      console.log(`üì¶ Modelos disponibles: ${response.data.models?.map(m => m.name).join(', ')}`);
      return true;
    } catch (error) {
      console.error('‚ùå Ollama no est√° disponible:', error.message);
      return false;
    }
  }
}

module.exports = OllamaIntegration;